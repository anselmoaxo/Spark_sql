# Primeiro Projeto com PySpark ğŸš€

Este repositÃ³rio contÃ©m meu primeiro projeto utilizando **PySpark**, desenvolvido com **Jupyter Notebook** para manipulaÃ§Ã£o e anÃ¡lise de dados distribuÃ­dos.

## ğŸ¯ Objetivo

Explorar o uso do PySpark para:
- Processamento de dados em larga escala.
- TransformaÃ§Ãµes e agregaÃ§Ãµes.
- VisualizaÃ§Ã£o de dados em notebooks.

## ğŸ—‚ Estrutura do RepositÃ³rio

- `notebooks/`: ContÃ©m os notebooks Jupyter com as anÃ¡lises e transformaÃ§Ãµes.
- `data/`: Dados de exemplo ou fictÃ­cios utilizados no projeto.
- `requirements.txt`: Lista de dependÃªncias para rodar os notebooks.
- `README.md`: DocumentaÃ§Ã£o do projeto.
- `.gitignore`: Arquivos/diretÃ³rios ignorados pelo Git.

## ğŸ›  Requisitos

- Python 3.8+
- PySpark
- Jupyter Notebook

## Dataset Utilizado

O dataset de empresas utilizado neste projeto pode ser baixado pelo link abaixo:

Receita Federal

- [Empresas Dataset](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/empresas.zip)
- [Estabelecimentos Dataset](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/estabelecimentos.zip)
- [Socios Dataset]( https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/socios.zip)


### InstalaÃ§Ã£o das DependÃªncias

1. Crie um ambiente virtual (recomendado):
   ```bash
   python -m venv venv
   source venv/bin/activate  # Linux/Mac
   venv\Scripts\activate     # Windows





