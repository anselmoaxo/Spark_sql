# Primeiro Projeto com PySpark 🚀

Este repositório contém meu primeiro projeto utilizando **PySpark**, desenvolvido com **Jupyter Notebook** para manipulação e análise de dados distribuídos.

## 🎯 Objetivo

Explorar o uso do PySpark para:
- Processamento de dados em larga escala.
- Transformações e agregações.
- Visualização de dados em notebooks.

## 🗂 Estrutura do Repositório

- `notebooks/`: Contém os notebooks Jupyter com as análises e transformações.
- `data/`: Dados de exemplo ou fictícios utilizados no projeto.
- `requirements.txt`: Lista de dependências para rodar os notebooks.
- `README.md`: Documentação do projeto.
- `.gitignore`: Arquivos/diretórios ignorados pelo Git.

## 🛠 Requisitos

- Python 3.8+
- PySpark
- Jupyter Notebook

## Dataset Utilizado

O dataset de empresas utilizado neste projeto pode ser baixado pelo link abaixo:

Receita Federal

- [Empresas Dataset](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/empresas.zip)
- [Estabelecimentos Dataset](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/estabelecimentos.zip)
- [Socios Dataset]( https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/socios.zip)


### Instalação das Dependências

1. Crie um ambiente virtual (recomendado):
   ```bash
   python -m venv venv
   source venv/bin/activate  # Linux/Mac
   venv\Scripts\activate     # Windows





